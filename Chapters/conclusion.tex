% Chapter 5

\chapter{Conclusion \& Perspectives}
\label{chapter:conclusion}

This thesis demonstrated various ways to solve the MEG/EEG source localization problem. It tackles specific challenges faced by current state-of-the-art techniques, and tries to improve them point by point:
\begin{itemize}
\item Promoting structured sparsity in the TF domain has proved useful for reconstructing non-stationary sources, although it needs to fix some parameters related to the Gabor transform, which are involved in the TF resolution. The first improvement proposed in this thesis was to tackle the choice of these parameters, which can be very detrimental for the analysis of brain waves with variable TF characteristics. It provides a new technique based on a multi-scale TF mixed norm allowing us to more accurately localize the source estimated in space and time (see Chapter~\ref{chapter:multiscale}).

\item The formulation of the MEG/EEG inverse problem has been mostly written as a penalized regression, meaning that it needs to introduce a prior knowledge as a regularization term into the objective function. This results in adding a hyperparameter to the model which needs to be tuned. This thesis tackles this second challenge in two ways, both reformulating the problem as done in the Bayesian community. The Bayesian formulation allows to hierarchically add hyperparameters that are alternatively estimated with the main parameters of the model (the sources). The two main advantages are: the direct estimation of the hyperparameters, and the ability to use sampling in order to investigate the uncertainty of these solvers. These two points were presented in Chapter~\ref{chapter:bayesian}.

\item An important step after developing any new technique is to validate it with a comparison with the other existing methods. This has for a long been a hard step as it is always hard to develop good and realistic simulations. For this aim, several studies have been investigating phantom datasets, which consists of real data recorded with a device mimicing a human head with focal sourdes. Chapter~\ref{chapter:benchmark} shows a comparison of the solvers presented in this thesis on multiple phantom datasets.
\end{itemize}

This thesis was based on a long line of research started by my supervisor Alexandre Gramfort, and then his former PhD student Daniel Strohmeier, and was meant to address several issues they encountered in the context of the MEG/EEG source localization problem. The points cited before were mainly the ones developed in this thesis, however several non-trivial ones remain in order to make best use of the available neuroimaging data:
\begin{itemize}
\item Although we found a way to solve the problem of source localization in the TF domain when having a mixture of brain signals in the data, it is still a non-trivial task to set the parameters of the multi-scale dictionary. As presented before, another possible way is to learn models that are good enough to capture the rich frequency content, and the morphology of the brain signal. The dictionary learning research line has given pretty nice results so far on electrophisiological signals~\cite{jas2017learning,jost2006motif,brockmeier2016learning,hitziger2017adaptive}, which makes the technique completely autonomous and data-driven.

\item The spatio-temporal techniques presented in this thesis are designed for the analysis of averaged evoked responses to ensure a descent SNR. Future work can be directed on how to make these techniques applicable on single trial data. A possible idea is to localize each trial separately by imposing an additional constraint onto the model, such as that the active set must be consistent over all trials~\cite{strohmeier2012meg,strohmeier2012biomag}.

\item While invoking new constraints onto the model, another line of future work can be on the optimization side for solving the MEG/EEG inverse problem. In machine learning, various papers have been investigating mathematical and computational challenges to better tackle the inverse problem in general. One possible direction for the MEG/EEG inverse problem is to improve the computational complexity, because the proposed approaches need to be competitive in terms of running time. This results in research contributions that aim to accelerate the optimization algorithms; a practical example which was used in this thesis is the use of an active set. A more sophisticated approach would be to apply screening rules, \textit{i.e.} find in advance the involved sources in order to compute the solution only for them, and avoid spending time on computing sources which will be inactive at the end~\cite{fercoq-etal:2015,massias2017safe,massiasgap,ndiaye2016gap,ndiaye2017efficient}.

\item The proposed method in the TF domain still lacks an automatic model selection criterion to set the two regularization hyperparameters (one over space, the second over time). Chapter~\ref{chapter:bayesian} presented a way to automatically set the hyperparameter for the mixed norm (MxNE) approach, which has only one regularization parameter over space. A future work could be to rewrite the problem for TF-MxNE, or investigate other model selection criteria.

\item The novel methods and some of state-of-the-art approaches have been tested using three phantom datasets. A future work would be to investigate more in depth this validation to compare their capabilities with a more sophisticated data, \textit{i.e.}, two or more dipoles at a time, instead of only one dipole as presented here.

\end{itemize}

\cleardoublepage